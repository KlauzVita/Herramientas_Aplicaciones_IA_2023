{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# <center> <font color= #000047> Módulo 2: Redes Neuronales Librería\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- La derivación es el aspecto algorítmicamente más costoso en la utilización de clasificadores basados en redes neuronales. \n",
    "\n",
    "- Calcular las decenas, centenares o centenares de miles de derivadas parciales sería una tarea abrumadora si la tuviéramos que resolver a mano cada vez que construimos una red neuronal. \n",
    "\n",
    "- Afortunadamente contamos con herramientas que hacen eficientemente esa labor por nosotros. **Keras** es una de ellas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Keras\n",
    "\n",
    "[Keras](https://keras.io/) es una API escrita en Python que nos permite de una forma rápida y cómoda configurar y entrenar redes neuronales.\n",
    "\n",
    "<img src=\"Figures/keras-logo.png\" width=\"30%\">\n",
    "\n",
    "- Hay otra cosa que Keras hace por nosotros y que es sumamente importante: trasladar el cálculo a la GPU en lugar de hacerlo en la CPU. Todo el cálculo que realiza la red para generar una salida es computacionalmente muy alto. Son muchísimas las multiplicaciones y sumas que se llevan a cabo. Pero, afortunadamente, la inmensa mayoría de estas operaciones son paralelizables. Y, de la misma forma que los juegos actuales utilizan la GPU para poder mover rápidamente una inmensa cantidad de puntos, vértices y polígonos, esta misma arquitectura de computación paralela se adapta perfectamente a las necesidades de cálculo de las redes neuronales.\n",
    "\n",
    "- Por supuesto, cuando las redes y sus conjuntos de datos son pequeños no es indispensable disponer de GPU en el ordenador. Pero en cuanto el modelo o los datos crecen el tiempo de cómputo se vuelve crucial.\n",
    "\n",
    "**Keras, a su vez, se apoya sobre otras herramientas como [Tensorflow](https://www.tensorflow.org/) y [CUDA](https://developer.nvidia.com/cuda-zone) (si disponemos de GPU).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación\n",
    "\n",
    "Visita [https://keras.io/#installation](https://keras.io/#installation) para instalar Keras. Antes debes [instalar Tensorflow](https://www.tensorflow.org/install). \n",
    "\n",
    "**Modo rápido:** Si quieres, puedes realizar una instalación limpia de Keras mediante un entorno virtual (recomendable). \n",
    "\n",
    "**Instala ambiente virtual con Anaconda**\n",
    "\n",
    "Desde consola:\n",
    "\n",
    "<code>conda create -n nombre_de_tu_entorno</code>\n",
    "\n",
    "Crea el entorno virtual\n",
    "\n",
    "<code>conda create -n nombre_de_tu_entorno python=3.6</code>\n",
    "\n",
    "Activa el entorno virtual\n",
    "\n",
    "<code>conda activate nombre_de_tu_entorno<\\code>\n",
    "\n",
    "Dentro del entorno, instala Tensorflow:\n",
    "\n",
    "<code>pip install tensorflow</code>\n",
    "\n",
    "y luego, Keras:\n",
    "\n",
    "<code>pip install keras</code>\n",
    "\n",
    "Con esto tendrás una instalación de Keras para realizar las prácticas, pero sin GPU. Si tu ordenador no tiene GPU, entonces será la opción adecuada. \n",
    "    \n",
    "Podemos instalar todas las librerías que necesitemos como pandas, numpy, matplotlib\n",
    "\n",
    "<code>pip install keras</code>\n",
    "<code>pip install pandas</code>\n",
    "<code>pip install matplotlib</code>\n",
    "\n",
    "**Añadir el Ambiente virtual a Jupyter Notebook**\n",
    "\n",
    "Jupyter Notebook se asegura de que el kernel de IPython esté disponible, pero se debe agregar manualmente un kernel con una versión diferente de Python o un entorno virtual. \n",
    "    \n",
    "-Primero, necesitamos activar su entorno virtual. \n",
    "    \n",
    "-A continuación, instalaremos ipykernel, que proporciona el kernel de IPython para Jupyter:\n",
    "\n",
    "<code>pip install --user ipykernel</code>\n",
    "\n",
    "Ahora se pude añadir un ambiente virtual a jupyter con el siguiente comando:\n",
    "\n",
    "<code>python -m ipykernel install --user --name=myenv</code>\n",
    "\n",
    "Ahora podemos elegir el entorno conda como Kernel en Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "La estructura principal de Keras es el **modelo**, lo cual es una forma de organizar y conectar capas de neuronas. El tipo de modelo más simple es el **modelo secuencial**, que es una pila lineal de capas. Para arquitecturas más complejas, es necesario utilizar la **API funcional** de Keras, que permite crear capas con conexiones arbitrarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora apilaremos capas con <code>.add()</code>. Keras denomina a las capas neuronales básicas como **densas** <code>Dense</code>, lo que significa que todas las entradas son conectadas a todas las neuronas. Como observamos en la figura siguiente, todas las $n$ entradas se conectan a todas las $m$ neuronas. Veremos más adelante que esto no siempre es así. Hay capas denominadas **convolutivas** que no siguen este patrón de conexión, sino que parte de las entradas se conectan solo a algunas neuronas de la capa. Pero, por ahora, eso es otra historia.\n",
    "\n",
    "<img src=\"Figures/densa.svg\" width=\"30%\">\n",
    "\n",
    "Fíjate que en la instanciación de la primera capa densa tenemos que especificar el número de entradas <code>input_dim=4</code>, pero en la siguiente capa no. Keras sabe que son $5$ entradas puesto que en la capa anterior hay $5$ neuronas <code>units=5</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar dense\n",
    "\n",
    "# capa con 4 entradas\n",
    "\n",
    "# capa con 5 neuronas\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del entrenamiento\n",
    "\n",
    "Una vez configurado el modelo, especificaremos el proceso de aprendizaje.\n",
    "\n",
    "- La **función de error** o **pérdida** (**loss**) que utilizaremos es <code>loss='mse'</code>, lo que significa *mean squared error* o error cuadrático medio. Es parecida a la suma de todos los errores que ya hemos visto pero dividido por el número de muestras sobre las que calculamos el error. \n",
    "\n",
    "- En cuando al optimizador, usaremos el clásico gradiente descendente <code>optimizer=keras.optimizers.SGD(lr=1)</code> (stochastic gradient descent) al que le aplicamos un *learning rate* o **tasa de aprendizaje** de $1$. \n",
    "\n",
    "\\- Lo de *gradiante descendente*, ya lo sabemos. Pero, ¿lo de *estocástico* qué significa? \n",
    "\n",
    "En este contexto, **estocástico** significa que no vamos a calcular el error sobre todo el conjunto de muestras en cada iteración sino sobre un subconjunto **aleatorio** de ellos. Ese subconjunto corresponde al *mini-batch* que ya vimos.\n",
    "\n",
    "Durante el proceso de entrenamiento vamos progresivamente descendiendo por la función de error hasta llegar a algún mínimo. Para verificar que vamos avanzando adecuadamente es conveniente ir comprobando cómo el valor de la función de error va bajando (*loss*). Sin embargo, *loss* es un valor que no indica nada en sí, solo que si baja es buena señal. Pero, lo que realmente nos va diciendo cuánto vamos mejorando es el *accuracy* (precisión) <code>metrics=['accuracy']</code>. Este valor se calcula introduciendo las muestras en la red y comprobando qué porcentaje de ellas ha clasificado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilar el modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de los datos. Entrenamiento y test\n",
    "\n",
    "Una vez completamente definido el modelo y cómo lo vamos a entrenar es necesario preparar los datos sobre los que vamos a trabajar. Esta vez vamos a dividir nuestro conjunto de muestras en dos subconjuntos: uno  para **entrenar** (*train*) y otro para **verificar** (*test*). Cuando entrenamos una red neuronal con un conjunto de muestras, ¿cómo podemos estar seguros de que esa red es capaz de clasificar correctamente nuevas muestras que nunca haya visto antes? Dicho de otro modo, ¿cómo podemos saber si la red puede **generalizar**?\n",
    "\n",
    "Para saber si una red ha aprendido correctamente hacemos esta división. Vamos a entrenar la red con el **conjunto de entrenamiento**. Y una vez entrenada comprobaremos cuántas muestras del **conjunto de test** es capaz de clasificar correctamente. Si el porcentaje de aciertos es satisfactorio concluimos que la red está correctamente entrenada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset iris\n",
    "\n",
    "\n",
    "# Etiquetas en formato one-hot\n",
    "\n",
    "# Particionando la data Tomamos el 80% (120) de las muestras para entrenar y el 20% (30) para testear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desordenar el dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partición de las muestras 80% para el train, y el 20% test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Procedemos a hacer el entrenamiento de la red. Para ello solo tenemos que invocar al método <code>fit</code> del modelo y especificarle que queremos entrenar $200$ épocas y que use un tamaño de lote de $15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit del modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización\n",
    "\n",
    "El objeto <code>history</code> que nos devuelve el método <code>fit</code> contiene la información acerca del progreso del entrenamiento. Vemos cómo el valor de *loss* va decreciendo mientras el *accuracy* va aproximándose a $1$, lo que representa casi el 100% de las muestras de entramiento bien clasificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualización de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Durante el entrenamiento *accuracy* nos indica el porcentaje de aciertos sobre el mismo conjunto de entrenamiento. Pero nos interesa conocer el porcentaje de acierto sobre un conjunto no visto antes por la red. Para ello utilizamos el conjunto de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test de la clasificación\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción\n",
    "\n",
    "Una vez entrenada y testeada la red, podemos ponerla en producción. Keras tiene funciones para guardar tanto el modelo como los pesos ya entrenados. Si queremos hacer una clasifiación invocaremos el método <code>predict</code> del modelo.\n",
    "\n",
    "Vamos a ver qué resultados nos ofrece la red si introducimos el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicción \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "- Varía algunos hiperparámetros (learning rate, tamaño del mini-lote, número de neuronas en la capa oculta, número de épocas) y observa qué ocurre.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
