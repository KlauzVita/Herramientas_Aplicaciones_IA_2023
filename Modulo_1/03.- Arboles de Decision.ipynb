{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52abda8b",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "\n",
    "# <center> <font color= #000047>  Módulo 1: Arboles de Desición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a588b",
   "metadata": {},
   "source": [
    "## Arboles de Desición\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fa6b1",
   "metadata": {},
   "source": [
    "Los árboles de decisión, también conocidos como modelos de árbol de clasificación y regresión (CART), son métodos basados en árboles para el aprendizaje automático supervisado. Los árboles de clasificación y de regresión simples son fáciles de usar e interpretar, pero no son competitivos con los mejores métodos de aprendizaje automático. Sin embargo, forman la base para el conjunto de modelos de ensamblaje como “bagged trees”, “random forest” y “boosted trees”, que aunque son menos interpretables, son muy precisos.\n",
    "\n",
    "Los modelos CART se puede definir en dos tipos de problemas\n",
    "\n",
    "**Árboles de clasificación:** la variable resultado es categórica y el métodos se utiliza para identificar la “clase” dentro de la cual es más probable que caiga nuestra variable resultado. Un ejemplo de un problema de tipo clasificación sería determinar quién se suscribirá o no a una plataforma digital; o quién se graduará o no de la escuela secundaria; o si una persona tiene cáncer o no.\n",
    "\n",
    "**Árboles de regressión:** la variable resultado es continua y el métodos se utiliza para predecir su valor. Un ejemplo de un problema de tipo regresión sería predecir los precios de venta de una casa residencial o el nivel de colesterol de una persona.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad515c2",
   "metadata": {},
   "source": [
    "Un árbol de decisión es una secuencia de operadores relacionales organizados como árbol donde:\n",
    "\n",
    "- Los atributos de un dato son evaluados desde la raiz hasta las hojas\n",
    "- Los nodos hoja (terminales) están asociados a una clase\n",
    "- Los nodos no-hoja están asociados a un operador lógico que divide los datos en dos o más conjuntos\n",
    "- El operador lógico o *split* se aplica sobre un atributo (feature) de los datos\n",
    "\n",
    "El siguiente diagrama ejemplifica el funcionamiento del árbol de decisión sobre un dataset con dos etiquetas y dos atributos (X y Z). \n",
    "\n",
    "<img src=\"Figures/tree.png\" width=\"600\">\n",
    "\n",
    "- La figura izquierda muestra un árbol de decisión binario con 5 nodos: 3 nodos hoja y 2 nodos de decisión.\n",
    "- La figura derecha muestra la partición que produce el árbol de decisión en el espacio de los datos. \n",
    "- Las separaciones o *splits* son siempre perpendiculares a los ejes de los datos (atributos).\n",
    "\n",
    "\n",
    "Entrenar el árbol de decisión es el proceso de escoger los atributos, operadores y umbrales de separación en los nodos de decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52c53b",
   "metadata": {},
   "source": [
    "La función de costo más común para los árboles de regresión es la suma de los residuos al cuadrado,\n",
    "\n",
    "$$RSS = \\sum_{k=1}^{K}\\sum_{i \\ in A_k} (y_i - \\hat{y}_{A_k})^2$$\n",
    "\n",
    "Para árboles de clasificación, es el índice de Gini,\n",
    "\n",
    "$$ G=\\sum_{c=1}^C \\hat{p}_{kc} (1-\\hat{p}_{kc})$$\n",
    "\n",
    "y la entropía (información estadística)\n",
    "\n",
    "$$ E= - \\sum_{c=1}^C \\hat{p}_{kc} log(\\hat{p}_{kc})$$\n",
    "\n",
    "dónde $\\hat{p}_{kc}$ es la proporción de observaciones de entrenamiento en el nodo $k$ que son de clase $c$. Un nodo completamente puro en un árbol binario tendría $\\hat{p} \\in \\{0,1\\}$ y $G=E=0$.  Un nodo completamente impuro en un árbol binario tendría $\\hat{p}=0.5$ y $G=0.5^2*2 = 0.25$, y $D=- (0.5 \\cdot log(0.5))\\cdot 2 = 0.69$\n",
    "\n",
    "La ganacia de información para un nodo que separa un conjunto de datos $D$ en dos $D_{izq}$ y $D_{der}$ es\n",
    "\n",
    "$$\n",
    "G(D; D_{izq}, D_{der}) = H(D) - \\frac{|D_{izq}|}{|D|} H(D_{izq}) - \\frac{|D_{der}|}{|D|} H(D_{der})\n",
    "$$\n",
    "\n",
    "donde $|A|$ es la cardinalidad del subconjunto $A$ y \n",
    "\n",
    "$$\n",
    "H(A) = - \\sum_{y \\in \\mathcal{Y}} p(y|A) \\log p(y|A)\n",
    "$$\n",
    "\n",
    "es la entropía del subconjunto $A$. En la expresión anterior $p(y|A)$ es la frecuencia relativa de los ejemplos de clase $y$ dentro de $A$.\n",
    "\n",
    "**La entropía mide la “pureza” del subconjunto en términos de sus clases. El subconjunto más puro es aquel donde todos los elementos son de la misma clase. El nodo más impuro es aquel en donde hay igual cantidad de elementos de cada clase (uniforme).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1790cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00a7764d",
   "metadata": {},
   "source": [
    "## Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d92e4",
   "metadata": {},
   "source": [
    "Sea el siguiente arreglo las etiquetas de un subconjunto de 12 ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4f965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "324d1f4a",
   "metadata": {},
   "source": [
    "Asumiendo que el problema sólo tiene dos clases las frecuencias relativas son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecf543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93a4df35",
   "metadata": {},
   "source": [
    "Y la entropía del conjunto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e234a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54bd72f2",
   "metadata": {},
   "source": [
    "La entropía es máxima si hay igual cantidad de ejemplos de ambas clases (mínima pureza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36623a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fb824ea",
   "metadata": {},
   "source": [
    "y mínima si todos los ejemplos son de una clase  (máxima pureza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf590fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c83102f",
   "metadata": {},
   "source": [
    "**Extensión a más de dos clases**\n",
    "\n",
    "Si un nodo separa el conjunto en $k$ subconjuntos la regla es\n",
    "\n",
    "\n",
    "- En cada nodo se escoge el atributo que maximiza la ganancia de información.\n",
    "\n",
    "Consideremos el siguiente problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d965f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "543d293b",
   "metadata": {},
   "source": [
    "donde queremos obtener un árbol de decisión que prediga el tiempo en función de la humedad y de la temperatura.\n",
    "\n",
    "Para decidir cual variable debe ir en el primer nodo comparamos sus ganancias de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacebe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef04efe6",
   "metadata": {},
   "source": [
    "Temperatura tiene mayor ganancia que humedad, por lo tanto el primer nodo separador utiliza temperatura.\n",
    "\n",
    "Si separamos por temperatura tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f9cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b2cd392",
   "metadata": {},
   "source": [
    "En el caso `frio` se produce un nodo con un sólo ejemplo. El algoritmo no seguirá dividiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db026c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82fdaadc",
   "metadata": {},
   "source": [
    "En el caso `caluroso` se produce un nodo con \"puro\". El algoritmo no seguirá dividiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba37311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c825a709",
   "metadata": {},
   "source": [
    "En el caso `templado` el nodo no es puro, debemos nuevamente escoger un atributo para separar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6509db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e07b60b",
   "metadata": {},
   "source": [
    "Por lo tanto se escoge humedad, lo cual produce dos nodos puros (con un sólo ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de63396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3cfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e85af64c",
   "metadata": {},
   "source": [
    "El algoritmo sigue separando el dataset de forma recursiva hasta que todos los nodos sean puros o hasta que se supere una profundidad máxima previamente designada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf4c14",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6d9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "547530a8",
   "metadata": {},
   "source": [
    "## Creación de la clase de Arboles de Desicion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_features=n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # check the stopping criteria\n",
    "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # calculate the information gain\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # calculate the weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        # calculate the IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9ec27",
   "metadata": {},
   "source": [
    "## Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d310a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05d4aaf8",
   "metadata": {},
   "source": [
    "## Ejemplo 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914bd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5e069b",
   "metadata": {},
   "source": [
    "## Implementación en scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a76c3",
   "metadata": {},
   "source": [
    "El módulo [`tree`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) de scikit-learn tiene implementaciones de árboles de decisión para problemas de clasificación y regresión. Nos enfocaremos en la primera.\n",
    "\n",
    "Los principales argumentos de [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) son:\n",
    "\n",
    "- `criterion`: El criterio que se utiliza para escoger los *splits*, las opciones son `'gini'` y `'entropy'`\n",
    "- `max_depth`: Límite para la profundidad máxima del árbol\n",
    "- `min_samples_split`: El número mínimo de ejemplos en un nodo para realizar un *split*\n",
    "- `min_samples_leaf`: El número mínimo de ejemplos que pueden estar en un nodo hoja\n",
    "- `min_impurity_decrease`: La disminución de pureza mínima en un nodo para realizar un *split*\n",
    "- `class_weight`: Permite asignar ponderación a las clases, es de utilidad si se tienen clases medianamente desbalanceadas\n",
    "- `max_features`: El número máximo de atributos a considerar en cada *split*\n",
    "\n",
    "\n",
    "Si se utilizan los argumentos (hiperparámetros) por defecto el árbol crecera hasta que sus nodos sean todos puros. Esto en general produce árboles de gran profundidad (muy capaces de sobreajustarse). \n",
    "\n",
    "Se puede limitar el tamaño de un árbol aumentando `min_samples_leaf` y/o `min_samples_split`, o disminuyendo `max_depth`.\n",
    "\n",
    "\n",
    "\n",
    "Los principales métodos son:\n",
    "\n",
    "- `predict(X)`: Retorna la clase predicha\n",
    "- `predict_proba(X)`: Retorna las probabilidades de pertenecer a cada una de las clases\n",
    "- `score(X,y)`: Retorna el *accuracy* de clasificación\n",
    "- `get_params()`: Retorna los nombres de los parámetros\n",
    "\n",
    "Además tiene algunos métodos no compartidos con otros estimadores como\n",
    "\n",
    "- `get_depth()`: Retorna la profunidad del árbol aprendido\n",
    "- `get_n_leaves()`: Retorna la cantidad de nodos hoja del árbol aprendida\n",
    "- `apply(X)`: Retorna el índice de la hoja que predice cada ejemplo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49f9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b10bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "522effce",
   "metadata": {},
   "source": [
    "Podemos utilizar la función [`plot_tree`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree) para obtener una visualización del árbol de decisión. En cada nodo se muestra:\n",
    "\n",
    "- El atributo y umbral seleccionados.\n",
    "- El valor del criterio (índice de gini).\n",
    "- La cantidad de ejemplos que entraron al nodo.\n",
    "- La cantidad de ejemplos que entraron al nodo separados por clase (en este caso tres).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391bedd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
